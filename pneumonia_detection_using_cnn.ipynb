{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd4c228-da60-4c3b-ba6c-e63f85746a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 28.31055212020874 seconds\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_image(img_path, output_folder, i, a):\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    h, w = image.shape\n",
    "\n",
    "    x_coords = np.arange(w)\n",
    "    x_matrix = np.tile(x_coords, (h, 1))\n",
    "    y_matrix = np.tile(np.arange(h).reshape(h, 1), (1, w))\n",
    "    distance_matrix = np.abs(x_matrix - w / 2)\n",
    "    mask = (y_matrix > a * distance_matrix ** 2).astype(np.uint8) * 255\n",
    "    result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, '{}.jpeg'.format(i)), result)\n",
    "\n",
    "# Input and output folders\n",
    "input_folder = r\"test\"\n",
    "output_folder = r\"test_output\"\n",
    "\n",
    "# List of subfolders (NORMAL and PNEUMONIA)\n",
    "subfolders = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process images for each subfolder using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for subfolder in subfolders:\n",
    "        input_path = os.path.join(input_folder, subfolder)\n",
    "        output_path = os.path.join(output_folder, subfolder)\n",
    "\n",
    "        # Create the output subfolder if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Load and process images for the current subfolder in parallel\n",
    "        image_paths = glob.glob(os.path.join(input_path, '*.jpeg'))\n",
    "        params = [(img, output_path, i, 0.0025) for i, img in enumerate(image_paths)]\n",
    "        executor.map(lambda p: process_image(*p), params)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233af46a-3bbe-46a0-90d8-71568a62607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 213.2499566078186 seconds\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_image(img_path, output_folder, i, a):\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    h, w = image.shape\n",
    "\n",
    "    x_coords = np.arange(w)\n",
    "    x_matrix = np.tile(x_coords, (h, 1))\n",
    "    y_matrix = np.tile(np.arange(h).reshape(h, 1), (1, w))\n",
    "    distance_matrix = np.abs(x_matrix - w / 2)\n",
    "    mask = (y_matrix > a * distance_matrix ** 2).astype(np.uint8) * 255\n",
    "    result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, '{}.jpeg'.format(i)), result)\n",
    "\n",
    "# Input and output folders\n",
    "input_folder = r\"train\"\n",
    "output_folder = r\"train_output\"\n",
    "\n",
    "# List of subfolders (NORMAL and PNEUMONIA)\n",
    "subfolders = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process images for each subfolder using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for subfolder in subfolders:\n",
    "        input_path = os.path.join(input_folder, subfolder)\n",
    "        output_path = os.path.join(output_folder, subfolder)\n",
    "\n",
    "        # Create the output subfolder if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Load and process images for the current subfolder in parallel\n",
    "        image_paths = glob.glob(os.path.join(input_path, '*.jpeg'))\n",
    "        params = [(img, output_path, i, 0.0025) for i, img in enumerate(image_paths)]\n",
    "        executor.map(lambda p: process_image(*p), params)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d1d6ef-c9ae-47de-9cec-b9e8755cc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3f4e31-49da-4b6f-a795-b96f60b39971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height = 224, 224\n",
    "num_of_class = 2\n",
    "\n",
    "sample_train = keras. utils.image_dataset_from_directory(\n",
    "    directory='train_output',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height))\n",
    "\n",
    "sample_test = keras. utils.image_dataset_from_directory(\n",
    "    directory='test_output',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(img_width, img_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57702b7-3dc9-4669-bb24-d0138a9f2da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "print(sample_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54537ca-e124-4d3b-8ebf-b4042bd52f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Extracting Features and Labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for feature, label in sample_train:\n",
    "    x_train.append(feature.numpy())\n",
    "    y_train.append(label.numpy())\n",
    "# Concatenate the lists to get the full 'x' and 'y' arrays\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62051d8e-692b-457f-95dd-447583973ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Extracting Features and Labels\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in sample_test:\n",
    "    x_test.append(feature.numpy())\n",
    "    y_test.append(label.numpy())\n",
    "# Concatenate the lists to get the full 'x' and 'y' arrays\n",
    "x_test = np.concatenate(x_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318b0871-66c7-4bfd-a825-9ff5643d9869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'x_train': (5216, 224, 224, 3)\n",
      "Shape of 'y_train': (5216, 2)\n",
      "Shape of 'x_test': (624, 224, 224, 3)\n",
      "Shape of 'y_test': (624, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of 'x_train':\", x_train.shape)\n",
    "print(\"Shape of 'y_train':\", y_train.shape)\n",
    "print(\"Shape of 'x_test':\", x_test.shape)\n",
    "print(\"Shape of 'y_test':\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff86d0ed-6b38-4555-8ee8-01a711e40fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6fb8896-cf54-45a9-9719-0752640f0910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 8)       608       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 8)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 112, 112, 8)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 16)      1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 56, 56, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 224)               11239648  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 224)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 450       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11241874 (42.88 MB)\n",
      "Trainable params: 11241874 (42.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers.pooling.max_pooling1d import MaxPool1D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5), padding = 'Same', activation= 'relu', input_shape = (224,224,3)))\n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(224, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "# Training The CNN\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "epochs = 30\n",
    "# Visualized Layers of CNN\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5682f9ad-5d25-4848-8a21-35de9e7de02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "163/163 [==============================] - 437s 2s/step - loss: 0.5688 - accuracy: 0.8092\n",
      "Epoch 2/30\n",
      "163/163 [==============================] - 165s 1s/step - loss: 0.2755 - accuracy: 0.8913\n",
      "Epoch 3/30\n",
      "163/163 [==============================] - 171s 1s/step - loss: 0.2481 - accuracy: 0.8982\n",
      "Epoch 4/30\n",
      "163/163 [==============================] - 372s 2s/step - loss: 0.2230 - accuracy: 0.9059\n",
      "Epoch 5/30\n",
      "163/163 [==============================] - 307s 2s/step - loss: 0.2058 - accuracy: 0.9156\n",
      "Epoch 6/30\n",
      "163/163 [==============================] - 177s 1s/step - loss: 0.1796 - accuracy: 0.9289\n",
      "Epoch 7/30\n",
      "163/163 [==============================] - 165s 1s/step - loss: 0.1726 - accuracy: 0.9373\n",
      "Epoch 8/30\n",
      "163/163 [==============================] - 155s 953ms/step - loss: 0.1640 - accuracy: 0.9377\n",
      "Epoch 9/30\n",
      "163/163 [==============================] - 156s 959ms/step - loss: 0.1603 - accuracy: 0.9388\n",
      "Epoch 10/30\n",
      "163/163 [==============================] - 157s 961ms/step - loss: 0.1426 - accuracy: 0.9456\n",
      "Epoch 11/30\n",
      "163/163 [==============================] - 158s 971ms/step - loss: 0.1367 - accuracy: 0.9473\n",
      "Epoch 12/30\n",
      "163/163 [==============================] - 159s 973ms/step - loss: 0.1335 - accuracy: 0.9498\n",
      "Epoch 13/30\n",
      "163/163 [==============================] - 155s 951ms/step - loss: 0.1099 - accuracy: 0.9594\n",
      "Epoch 14/30\n",
      "163/163 [==============================] - 156s 960ms/step - loss: 0.1075 - accuracy: 0.9592\n",
      "Epoch 15/30\n",
      "163/163 [==============================] - 157s 962ms/step - loss: 0.1128 - accuracy: 0.9567\n",
      "Epoch 16/30\n",
      "163/163 [==============================] - 155s 951ms/step - loss: 0.0918 - accuracy: 0.9638\n",
      "Epoch 17/30\n",
      "163/163 [==============================] - 155s 950ms/step - loss: 0.0925 - accuracy: 0.9661\n",
      "Epoch 18/30\n",
      "163/163 [==============================] - 156s 956ms/step - loss: 0.0819 - accuracy: 0.9674\n",
      "Epoch 19/30\n",
      "163/163 [==============================] - 158s 971ms/step - loss: 0.1203 - accuracy: 0.9536\n",
      "Epoch 20/30\n",
      "163/163 [==============================] - 164s 1s/step - loss: 0.0829 - accuracy: 0.9674\n",
      "Epoch 21/30\n",
      "163/163 [==============================] - 156s 955ms/step - loss: 0.0821 - accuracy: 0.9672\n",
      "Epoch 22/30\n",
      "163/163 [==============================] - 157s 962ms/step - loss: 0.0664 - accuracy: 0.9726\n",
      "Epoch 23/30\n",
      "163/163 [==============================] - 156s 959ms/step - loss: 0.0565 - accuracy: 0.9757\n",
      "Epoch 24/30\n",
      "163/163 [==============================] - 156s 958ms/step - loss: 0.0716 - accuracy: 0.9714\n",
      "Epoch 25/30\n",
      "163/163 [==============================] - 156s 960ms/step - loss: 0.0575 - accuracy: 0.9747\n",
      "Epoch 26/30\n",
      "163/163 [==============================] - 160s 979ms/step - loss: 0.0551 - accuracy: 0.9768\n",
      "Epoch 27/30\n",
      "163/163 [==============================] - 157s 963ms/step - loss: 0.0592 - accuracy: 0.9757\n",
      "Epoch 28/30\n",
      "163/163 [==============================] - 158s 967ms/step - loss: 0.0466 - accuracy: 0.9831\n",
      "Epoch 29/30\n",
      "163/163 [==============================] - 157s 964ms/step - loss: 0.0399 - accuracy: 0.9831\n",
      "Epoch 30/30\n",
      "163/163 [==============================] - 157s 961ms/step - loss: 0.0514 - accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30114aac-db26-4606-a833-4b08e49946e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 14s 234ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123531f4-3485-486a-97d4-ac145b9305c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data,threshold):\n",
    "  for i in range(len(data)):\n",
    "      for j in range(len(data[i])):\n",
    "          if data[i][j] < threshold:\n",
    "              data[i][j] = 0\n",
    "          else:\n",
    "              data[i][j] = 1\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f9a9a4-9151-40b3-b6b1-2139095e7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_01 = func(Y_pred,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f6b44f-7216-4ab7-be3e-90a1124f36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = func(y_test,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c595675-da6d-45ab-8510-8bd21fa04d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.30      0.45       234\n",
      "           1       0.70      0.98      0.82       390\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       624\n",
      "   macro avg       0.80      0.64      0.63       624\n",
      "weighted avg       0.78      0.73      0.68       624\n",
      " samples avg       0.73      0.73      0.73       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, multilabel_confusion_matrix\n",
    "\n",
    "print(classification_report(Y_true,Y_pred_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebd9bd86-3c9f-4c25-a1eb-c0f110c08b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the model: 72.59615384615384%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\n Accuracy of the model: {0}%\".format(accuracy_score(Y_true, Y_pred_01)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335502ba-1ab7-46a9-a564-eed30e3fb0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
